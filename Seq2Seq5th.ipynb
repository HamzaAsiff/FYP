{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "106b36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence,pack_padded_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "random.seed(1024)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a078e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b845262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c017ad",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e55232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is for Sequence 2 Sequence format\n",
    "def pad_to_batch(batch, x_to_ix, y_to_ix):\n",
    "    \n",
    "    sorted_batch =  sorted(batch, key=lambda b:b[0].size(1), reverse=True) # sort by len\n",
    "    x,y = list(zip(*sorted_batch))\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    max_y = max([s.size(1) for s in y])\n",
    "    x_p, y_p = [], []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1) < max_x:\n",
    "            x_p.append(torch.cat([x[i], Variable(LongTensor([x_to_ix['<PAD>']] * (max_x - x[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "        if y[i].size(1) < max_y:\n",
    "            y_p.append(torch.cat([y[i], Variable(LongTensor([y_to_ix['<PAD>']] * (max_y - y[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            y_p.append(y[i])\n",
    "        \n",
    "    input_var = torch.cat(x_p)\n",
    "    target_var = torch.cat(y_p)\n",
    "    input_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in input_var]\n",
    "    target_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in target_var]\n",
    "    \n",
    "    return input_var, target_var, input_len, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ba137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f45ef9",
   "metadata": {},
   "source": [
    "### Data load and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a58a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f7d97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('input-output.txt', 'r', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab4b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testCorpus = open('Dataset/1st/test.txt', 'r', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf708570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13161"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6d6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18648b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12268 12268\n",
      "['the', 'poor', 'orphans', 'life', 'hasnt', 'bed', 'roses', 'easy', 'means'] ['the', 'poor', 'orphans', 'life', 'hasnt', 'been', 'a', 'bed', 'of', 'roses', 'or', 'easy', 'by', 'any', 'means', '.']\n",
      "Wall time: 649 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_r, y_r = [], [] # raw\n",
    "\n",
    "for parallel in corpus:\n",
    "    so,ta = parallel[:-1].split('\\t')\n",
    "    if so.strip() == \"\" or ta.strip() == \"\": \n",
    "        continue\n",
    "    \n",
    "    normalized_so = normalize_string(so).split()\n",
    "    normalized_ta = normalize_string(ta).split()\n",
    "    \n",
    "    if len(normalized_so) >= MIN_LENGTH and len(normalized_so) <= MAX_LENGTH \\\n",
    "    and len(normalized_ta) >= MIN_LENGTH and len(normalized_ta) <= MAX_LENGTH:\n",
    "        X_r.append(normalized_so)\n",
    "        y_r.append(normalized_ta)\n",
    "    \n",
    "\n",
    "print(len(X_r), len(y_r))\n",
    "print(X_r[0], y_r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b02b6",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e317b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21938 21619\n"
     ]
    }
   ],
   "source": [
    "source_vocab = list(set(flatten(X_r)))\n",
    "target_vocab = list(set(flatten(y_r)))\n",
    "print(len(source_vocab), len(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9005241",
   "metadata": {},
   "outputs": [],
   "source": [
    "source2index = {'<PAD>': 0, '<UNK>': 1, '<s>': 2, '</s>': 3}\n",
    "for vo in source_vocab:\n",
    "    if source2index.get(vo) is None:\n",
    "        source2index[vo] = len(source2index)\n",
    "index2source = {v:k for k, v in source2index.items()}\n",
    "\n",
    "target2index = {'<PAD>': 0, '<UNK>': 1, '<s>': 2, '</s>': 3}\n",
    "for vo in target_vocab:\n",
    "    if target2index.get(vo) is None:\n",
    "        target2index[vo] = len(target2index)\n",
    "index2target = {v:k for k, v in target2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4124a",
   "metadata": {},
   "source": [
    "### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e2ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_p, y_p = [], []\n",
    "\n",
    "for so, ta in zip(X_r, y_r):\n",
    "    X_p.append(prepare_sequence(so + ['</s>'], source2index).view(1, -1))\n",
    "    y_p.append(prepare_sequence(ta + ['</s>'], target2index).view(1, -1))\n",
    "    \n",
    "train_data = list(zip(X_p, y_p))\n",
    "#print(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005282e",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5333d",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c851b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size,hidden_size, n_layers=1,bidirec=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        if bidirec:\n",
    "            self.n_direction = 2 \n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True, bidirectional=True)\n",
    "        else:\n",
    "            self.n_direction = 1\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "    \n",
    "    def init_hidden(self, inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers * self.n_direction, inputs.size(0), self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "        self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "        self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "    \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        \"\"\"\n",
    "        inputs : B, T (LongTensor)\n",
    "        input_lengths : real lengths of input batch (list)\n",
    "        \"\"\"\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        packed = pack_padded_sequence(embedded, input_lengths, batch_first=True)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True) # unpack (back to padded)\n",
    "                \n",
    "        if self.n_layers > 1:\n",
    "            if self.n_direction == 2:\n",
    "                hidden = hidden[-2:]\n",
    "            else:\n",
    "                hidden = hidden[-1]\n",
    "        \n",
    "        return outputs, torch.cat([h for h in hidden], 1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01894a28",
   "metadata": {},
   "source": [
    "### Decoder with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfc7f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1, dropout_p=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_size + hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, input_size)\n",
    "        self.attn = nn.Linear(self.hidden_size, self.hidden_size) # Attention\n",
    "    \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, inputs.size(0), self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "        self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "        self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "        self.linear.weight = nn.init.xavier_uniform(self.linear.weight)\n",
    "        self.attn.weight = nn.init.xavier_uniform(self.attn.weight)\n",
    "#         self.attn.bias.data.fill_(0)\n",
    "    \n",
    "    def Attention(self, hidden, encoder_outputs, encoder_maskings):\n",
    "        \"\"\"\n",
    "        hidden : 1,B,D\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        \"\"\"\n",
    "        hidden = hidden[0].unsqueeze(2)  # (1,B,D) -> (B,D,1)\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0) # B\n",
    "        max_len = encoder_outputs.size(1) # T\n",
    "        energies = self.attn(encoder_outputs.contiguous().view(batch_size * max_len, -1)) # B*T,D -> B*T,D\n",
    "        energies = energies.view(batch_size,max_len, -1) # B,T,D\n",
    "        attn_energies = energies.bmm(hidden).squeeze(2) # B,T,D * B,D,1 --> B,T\n",
    "        \n",
    "#         if isinstance(encoder_maskings,torch.autograd.variable.Variable):\n",
    "#             attn_energies = attn_energies.masked_fill(encoder_maskings,float('-inf'))#-1e12) # PAD masking\n",
    "        \n",
    "        alpha = F.softmax(attn_energies,1) # B,T\n",
    "        alpha = alpha.unsqueeze(1) # B,1,T\n",
    "        context = alpha.bmm(encoder_outputs) # B,1,T * B,T,D => B,1,D\n",
    "        \n",
    "        return context, alpha\n",
    "    \n",
    "    \n",
    "    def forward(self, inputs, context, max_length, encoder_outputs, encoder_maskings=None, is_training=False):\n",
    "        \"\"\"\n",
    "        inputs : B,1 (LongTensor, START SYMBOL)\n",
    "        context : B,1,D (FloatTensor, Last encoder hidden state)\n",
    "        max_length : int, max length to decode # for batch\n",
    "        encoder_outputs : B,T,D\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        is_training : bool, this is because adapt dropout only training step.\n",
    "        \"\"\"\n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(inputs)\n",
    "        hidden = self.init_hidden(inputs)\n",
    "        if is_training:\n",
    "            embedded = self.dropout(embedded)\n",
    "        \n",
    "        decode = []\n",
    "        # Apply GRU to the output so far\n",
    "        for i in range(max_length):\n",
    "\n",
    "            _, hidden = self.gru(torch.cat((embedded, context), 2), hidden) # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = torch.cat((hidden, context.transpose(0, 1)), 2) # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.linear(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score,1)\n",
    "            decode.append(softmaxed)\n",
    "            decoded = softmaxed.max(1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) # y_{t-1}\n",
    "            if is_training:\n",
    "                embedded = self.dropout(embedded)\n",
    "            \n",
    "            # compute next context vector using attention\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs, encoder_maskings)\n",
    "            \n",
    "        #  column-wise concat, reshape!!\n",
    "        scores = torch.cat(decode, 1)\n",
    "        return scores.view(inputs.size(0) * max_length, -1)\n",
    "    \n",
    "    def decode(self, context, encoder_outputs):\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']] * 1])).transpose(0, 1)\n",
    "        embedded = self.embedding(start_decode)\n",
    "        hidden = self.init_hidden(start_decode)\n",
    "        \n",
    "        decodes = []\n",
    "        attentions = []\n",
    "        decoded = embedded\n",
    "        while decoded.data.tolist()[0] != target2index['</s>']: # until </s>\n",
    "            _, hidden = self.gru(torch.cat((embedded, context), 2), hidden) # h_t = f(h_{t-1},y_{t-1},c)\n",
    "            concated = torch.cat((hidden, context.transpose(0, 1)), 2) # y_t = g(h_t,y_{t-1},c)\n",
    "            score = self.linear(concated.squeeze(0))\n",
    "            softmaxed = F.log_softmax(score,1)\n",
    "            decodes.append(softmaxed)\n",
    "            decoded = softmaxed.max(1)[1]\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) # y_{t-1}\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs,None)\n",
    "            attentions.append(alpha.squeeze(1))\n",
    "        \n",
    "        return torch.cat(decodes).max(1)[1], torch.cat(attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9558b6",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc905256",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "LR = 0.001\n",
    "DECODER_LEARNING_RATIO = 5.0\n",
    "RESCHEDULED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61091e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-2319a2dc2369>:23: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
      "<ipython-input-14-2319a2dc2369>:24: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
      "<ipython-input-14-2319a2dc2369>:25: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
      "<ipython-input-15-8e345673725b>:22: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
      "<ipython-input-15-8e345673725b>:26: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.attn.weight = nn.init.xavier_uniform(self.attn.weight)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(len(source2index), EMBEDDING_SIZE, HIDDEN_SIZE, 3, True)\n",
    "decoder = Decoder(len(target2index), EMBEDDING_SIZE, HIDDEN_SIZE * 2)\n",
    "encoder.init_weight()\n",
    "decoder.init_weight()\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=LR)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=LR * DECODER_LEARNING_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dba9251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-e21fa1c2666c>:17: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(encoder.parameters(), 50.0) # gradient clipping\n",
      "<ipython-input-19-e21fa1c2666c>:18: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(decoder.parameters(), 50.0) # gradient clipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/50] [000/191] mean_loss : 9.98\n",
      "[01/50] [000/191] mean_loss : 6.25\n",
      "[02/50] [000/191] mean_loss : 5.50\n",
      "[03/50] [000/191] mean_loss : 6.26\n",
      "[04/50] [000/191] mean_loss : 5.91\n",
      "[05/50] [000/191] mean_loss : 5.78\n",
      "[06/50] [000/191] mean_loss : 5.56\n",
      "[07/50] [000/191] mean_loss : 5.42\n",
      "[08/50] [000/191] mean_loss : 5.24\n",
      "[09/50] [000/191] mean_loss : 5.13\n",
      "[10/50] [000/191] mean_loss : 4.95\n",
      "[11/50] [000/191] mean_loss : 4.79\n",
      "[12/50] [000/191] mean_loss : 4.72\n",
      "[13/50] [000/191] mean_loss : 4.65\n",
      "[14/50] [000/191] mean_loss : 4.25\n",
      "[15/50] [000/191] mean_loss : 4.28\n",
      "[16/50] [000/191] mean_loss : 4.15\n",
      "[17/50] [000/191] mean_loss : 4.07\n",
      "[18/50] [000/191] mean_loss : 3.97\n",
      "[19/50] [000/191] mean_loss : 3.76\n",
      "[20/50] [000/191] mean_loss : 4.03\n",
      "[21/50] [000/191] mean_loss : 3.86\n",
      "[22/50] [000/191] mean_loss : 7.93\n",
      "[23/50] [000/191] mean_loss : 5.91\n",
      "[24/50] [000/191] mean_loss : 5.28\n",
      "[25/50] [000/191] mean_loss : 4.99\n",
      "[26/50] [000/191] mean_loss : 4.72\n",
      "[27/50] [000/191] mean_loss : 4.44\n",
      "[28/50] [000/191] mean_loss : 4.32\n",
      "[29/50] [000/191] mean_loss : 4.20\n",
      "[30/50] [000/191] mean_loss : 4.17\n",
      "[31/50] [000/191] mean_loss : 4.15\n",
      "[32/50] [000/191] mean_loss : 4.11\n",
      "[33/50] [000/191] mean_loss : 4.06\n",
      "[34/50] [000/191] mean_loss : 4.14\n",
      "[35/50] [000/191] mean_loss : 4.11\n",
      "[36/50] [000/191] mean_loss : 4.04\n",
      "[37/50] [000/191] mean_loss : 3.94\n",
      "[38/50] [000/191] mean_loss : 4.01\n",
      "[39/50] [000/191] mean_loss : 3.91\n",
      "[40/50] [000/191] mean_loss : 3.89\n",
      "[41/50] [000/191] mean_loss : 3.84\n",
      "[42/50] [000/191] mean_loss : 3.86\n",
      "[43/50] [000/191] mean_loss : 3.86\n",
      "[44/50] [000/191] mean_loss : 3.70\n",
      "[45/50] [000/191] mean_loss : 3.75\n",
      "[46/50] [000/191] mean_loss : 3.74\n",
      "[47/50] [000/191] mean_loss : 3.63\n",
      "[48/50] [000/191] mean_loss : 3.61\n",
      "[49/50] [000/191] mean_loss : 3.94\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        inputs, targets, input_lengths, target_lengths = pad_to_batch(batch, source2index, target2index)\n",
    "        \n",
    "        input_masks = torch.cat([Variable(ByteTensor(tuple(map(lambda s: s ==0, t.data)))) for t in inputs]).view(inputs.size(0), -1)\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']] * targets.size(0)])).transpose(0, 1)\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        output, hidden_c = encoder(inputs, input_lengths)\n",
    "        \n",
    "        preds = decoder(start_decode, hidden_c, targets.size(1), output, input_masks, True)\n",
    "                                \n",
    "        loss = loss_function(preds, targets.view(-1))\n",
    "        losses.append(loss.data.tolist())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(encoder.parameters(), 50.0) # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm(decoder.parameters(), 50.0) # gradient clipping\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "\n",
    "        if i % 200==0:\n",
    "            print(\"[%02d/%d] [%03d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, i, len(train_data)//BATCH_SIZE, np.mean(losses)))\n",
    "            losses=[]\n",
    "\n",
    "    # You can use http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate\n",
    "    if RESCHEDULED == False and epoch  == EPOCH//2:\n",
    "        LR *= 0.01\n",
    "        enc_optimizer = optim.Adam(encoder.parameters(), lr=LR)\n",
    "        dec_optimizer = optim.Adam(decoder.parameters(), lr=LR * DECODER_LEARNING_RATIO)\n",
    "        RESCHEDULED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "819941a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed code from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "\n",
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05521b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source :  motioning gatekeeper let in funeral director prepared take casket cemetery\n",
      "Truth :  motioning for the gatekeeper to let him in , the funeral director prepared to take the casket to the cemetery .\n",
      "Prediction :  motioning the the gatekeeper gatekeeper the the the the , the the the the the to to the the . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-b81d1e789605>:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + input_words, rotation=90)\n",
      "<ipython-input-20-b81d1e789605>:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + output_words)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = random.choice(train_data)\n",
    "input_ = test[0]\n",
    "truth = test[1]\n",
    "\n",
    "output, hidden = encoder(input_, [input_.size(1)])\n",
    "pred, attn = decoder.decode(hidden, output)\n",
    "\n",
    "input_ = [index2source[i] for i in input_.data.tolist()[0]]\n",
    "pred = [index2target[i] for i in pred.data.tolist()]\n",
    "\n",
    "\n",
    "print('Source : ',' '.join([i for i in input_ if i not in ['</s>']]))\n",
    "print('Truth : ',' '.join([index2target[i] for i in truth.data.tolist()[0] if i not in [2, 3]]))\n",
    "print('Prediction : ',' '.join([i for i in pred if i not in ['</s>']]))\n",
    "\n",
    "if USE_CUDA:\n",
    "    attn = attn.cpu()\n",
    "\n",
    "show_attention(input_, pred, attn.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
